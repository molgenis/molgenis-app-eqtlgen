<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Cookbook for eQTLGen phase II analyses - eQTLGen Phase II</title>

    

    <!-- Bootstrap core CSS -->
    <link rel="stylesheet" href="../css/vendor/bootstrap/bootstrap.min.css">

    <!-- Custom styles for this template -->
    <link rel="stylesheet" href="../css/phase2/eqtlgen.css">

    

    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">

</head>

<body>

<nav class="navbar fixed-top navbar-expand-lg navbar-dark bg-dark fixed-top">
    <div class="container">
        <a class="navbar-brand" href="../index.html">eQTLGen phase II</a>
        <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse"
                data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false"
                aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="navbarResponsive">
            <ul class="navbar-nav ml-auto">
                
                <li class="nav-item">
                    <a class="nav-link" href="../index.html">
                        Home
                    </a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="cohorts.html">
                        Consortium members
                    </a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="collaboration.html">
                        Collaborate
                    </a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="documents.html">
                        Documents
                    </a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="eQTLGen-p2-cookbook.html">
                        Cookbook
                    </a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="articles.html">
                        Publications
                    </a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="resources.html">
                        Recources
                    </a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="https://github.com/eQTLGen">
                        <span class="fa fa-github"></span>
                    </a>
                </li>
            </ul>
        </div>
    </div>
</nav>

<div class="container" style="margin-top: 75px">
    <h1 id="eqtlgen-phase-ii-cookbook">eQTLGen phase II cookbook</h1>

<p>This cookbook is for running genome-wide eQTL mapping in the consortium
settings, by using HASE (<a href="https://www.nature.com/articles/srep36076">Roschupkin et al,
2016</a>; <a href="https://github.com/roshchupkin/hase">original code
repo</a>). HASE method enables to run
of genome-wide high-dimensional association analyses in consortium
settings by limiting the size of the shared data while preserving
participant privacy.</p>

<p>In this cookbook we perform the required steps for performing eQTL
analysis: we do data QC, remove related samples, do genotype phasing and
imputation, convert genotype data to the needed format, prepare
covariates, convert expression and covariate data into the needed
format, encode the data, calculate partial derivatives, prepare files
for permuted data, and organize <em>non-personal</em> encoded data and partial
derivatives for sharing with the central site. We also include pipelines
that will be used for running meta-analyses on the central site.</p>

<p>For eQTLGen phase II we perform this analysis for <strong>whole blood</strong> and
<strong>PBMC</strong> tissues and in the individual datasets <strong>with at least 100
samples</strong>.</p>

<h3 id="why-this-method">Why this method?</h3>

<p>Classical meta-analysis requires that each cohort performs a full GWAS
for every gene (~20,000). Full summary statistics are large, meaning
that, per every cohort, <em>several terabytes</em> of data would be needed to
be shared with the central site. Sharing that much data is technically
very complicated.</p>

<p>In contrast, the HASE method calculates matrices of aggregate statistics
(called partial derivatives) which are needed for running predefined
models and, additionally, encoded genotype and expression matrices. All
those matrices are in the efficient file format and relatively small
size, making it feasible to share those. Encoding means performing a
matrix multiplication of the original data with the random matrix <em>F</em>
(or its inverse, in case of expression matrix), so that no person-level
information is obtainable from the genotype and expression matrices
after encoding. However, sharing partial derivatives and encoded
matrices with the central site enables running of pre-defined
association tests between every variant and every gene for every
dataset; and finally, running the meta-analysis over datasets. An
additional merit of HASE is that it is possible adaptively drop
covariates in the encoded association test (by slicing the aggregate
partial derivative matrices), therefore enabling to make informed
decisions on the most suitable set of covariates to include into the
final meta-analysis. Also, this method shifts most of the computational
burden into the central site, so individual cohorts do not need to run
~20,000 GWAS’es in their respective HPCs.</p>

<p>You can see the specifics of the method here: <a href="https://www.nature.com/articles/srep36076">Roschupkin et al,
2016</a></p>

<h3 id="technical-support">Technical support</h3>

<p>In case of any issues when running this cookbook please contact Urmo
Võsa (urmo.vosa at gmail.com) and Robert Warmerdam (c.a.warmerdam at
umcg.nl).</p>

<h3 id="prerequisites">Prerequisites</h3>

<h4 id="needed">Needed</h4>

<ol>
  <li>HPC with multiple cores, UNIX/Linux and scheduling system.</li>
  <li>You need to have Bash &gt;=3.2 installed to your HPC.</li>
  <li>You need to have Java &gt;=11 installed to your HPC. This is needed
for running Nextflow pipeline management tool.</li>
</ol>

<h4 id="strongly-recommended">Strongly recommended</h4>

<ol>
  <li>Our pipelines expect that you have
<a href="https://sylabs.io/guides/3.5/user-guide/../index.html#">Singularity</a>
configured and running in your HPC, for managing needed
tools/dependencies. This means that you don’t need to install many
programs to your HPC. If there is no way of using Singularity in
your HPC, please contact us and we will look into alternative ways
for dependency management.</li>
  <li>You might also need a few extra modules, so that Singularity would
run as expected. If this is the case, we recommend to check this
with your HPC documentation and/or tech support. E.g. in University
of Tartu HPC there is also module named <strong>squashFS</strong> needed, so that
Singularity would work correctly.</li>
  <li>You HPC should have access to the internet. This is needed for
downloading analysis pipelines from code repos, for automatic
download of the containers from container repos and for downloading
some reference files. If it is mandatory for you to work offline,
please use <a href="eQTLGen-p2-offline-instructions.html">these
instructions</a> for getting the
pipelines working offline. Please contact lead analysts in case
additional help is needed.</li>
</ol>

<h4 id="recommended">Recommended</h4>

<ol>
  <li><a href="https://git-scm.com/">git</a>. This is convenient for cloning analysis
pipelines from the code repos. However, you can also download those
manually.</li>
  <li>Current analysis pipelines and configurations in this cookbook are
tailored for usage with <a href="slurm.schedmd.com">Slurm</a> scheduler. We
also provide profiles for two other commonly used schedulers
(PBS/TORQUE and SGE; we have no possibility to test but these should
work). It is also straightforward to add configurations for other
commonly used schedulers. Therefore, if your HPC uses some other
scheduler, please contact us and we will try to make the
configuration file for that specific scheduler.</li>
</ol>

<h3 id="setup">Setup</h3>

<p>The analysis consists of four <a href="nextflow.io">Nextflow</a> pipelines which
are used to perform all needed tasks in the correct order. For each
pipeline, we provide a Slurm script template which should be
complemented/adjusted with your input paths and a few dataset-specific
settings. Those pipelines are:</p>

<ul>
  <li><a href="#1-data-qc">Data QC pipeline</a>: for doing automatic genotype and
gene expression quality control, constructing covariates, and
calculating some diagnostic summary statistics.</li>
  <li><a href="#2-genotype-imputation">Imputation pipeline</a>: for imputing the
quality-controlled genotype data to the common reference panel.</li>
  <li><a href="#3-genotype-conversion">Genotype conversion pipeline</a>: for
converting imputed genotype data into efficient <code class="language-plaintext highlighter-rouge">.hdf5</code> format.</li>
  <li><a href="#4-per-cohort-data-preparations">Per-cohort data preparations</a>: for
making per-cohort preparations, encoding data, calculating partial
derivatives, doing permutation, and organizing all needed files for
sharing with the central site.</li>
</ul>

<p>We will provide detailed instructions for each pipeline below, however
for the most optimal setup, we recommend you to first organize your
analysis folder as follows:</p>

<ul>
  <li>
    <p>Make analysis root folder for this project, e.g. <code class="language-plaintext highlighter-rouge">eQTLGen_phase2</code>.</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mkdir eQTLGen_phase2
</code></pre></div>    </div>
  </li>
  <li>
    <p><em>Inside</em> this folder make another folder for Nextflow executable,
called <code class="language-plaintext highlighter-rouge">tools</code>. This path is specified in all the template scripts,
so that scripts find Nextflow executable.</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mkdir eQTLGen_phase2/tools
</code></pre></div>    </div>
  </li>
  <li>
    <p><em>Inside</em> <code class="language-plaintext highlighter-rouge">eQTLGen_phase2/tools</code> download and self-install Nextflow
executable, as specified
<a href="https://www.nextflow.io/docs/latest/getstarted.html#installation">here</a>.
You might need to load Java &gt;=11 before running self-install
(e.g. <code class="language-plaintext highlighter-rouge">module load [Java &gt;=11 module name in your HPC]</code>).</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cd eQTLGen_phase2/tools
wget -qO- https://get.nextflow.io | bash
</code></pre></div>    </div>
  </li>
</ul>

<p>❗ If this does not work, it might be that your HPC has restrictions in
connecting with internet. If this is the case please use <a href="eQTLGen-p2-offline-instructions.html">these
instructions</a> for trying to
install Nextflow offline. Please contact lead analysts in case
additional help is needed.</p>

<ul>
  <li>
    <p><em>Inside</em> <code class="language-plaintext highlighter-rouge">eQTLGen_phase2</code>, make additional separate folders for each
step of this analysis plan:</p>

    <ul>
      <li><code class="language-plaintext highlighter-rouge">eQTLGen_phase2/1_DataQC</code></li>
      <li><code class="language-plaintext highlighter-rouge">eQTLGen_phase2/2_Imputation</code></li>
      <li><code class="language-plaintext highlighter-rouge">eQTLGen_phase2/3_ConvertVcf2Hdf5</code></li>
      <li><code class="language-plaintext highlighter-rouge">eQTLGen_phase2/4_PerCohortPreparations</code></li>
    </ul>

    <!-- -->

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cd ../
mkdir 1_DataQC 2_Imputation 3_ConvertVcf2Hdf5 4_PerCohortPreparations
</code></pre></div>    </div>
  </li>
  <li>
    <p><em>Inside</em> each of those folders, you should clone/download the
corresponding pipeline. If git is available in your HPC, easiest is
to use command <code class="language-plaintext highlighter-rouge">git clone [repo of the pipeline]</code>. This yields a
pipeline folder e.g. for the first pipeline it will look like that:
<code class="language-plaintext highlighter-rouge">eQTLGen_phase2/1_DataQC/DataQC</code>.</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cd 1_DataQC
git clone https://github.com/eQTLGen/DataQC.git
cd ../2_Imputation
git clone https://github.com/eQTLGen/eQTLGenImpute.git
cd ../3_ConvertVcf2Hdf5
git clone https://github.com/eQTLGen/ConvertVcf2Hdf5.git
cd ../4_PerCohortPreparations
git clone https://github.com/eQTLGen/PerCohortDataPreparations.git
</code></pre></div>    </div>
  </li>
  <li>
    <p>We recommend to specify <code class="language-plaintext highlighter-rouge">output</code> (and, if needed, <code class="language-plaintext highlighter-rouge">input</code> etc.)
folder(s) for each step. E.g. <code class="language-plaintext highlighter-rouge">eQTLGen_phase2/1_DataQC/output</code>.</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cd ..
mkdir 1_DataQC/output 2_Imputation/output 3_ConvertVcf2Hdf5/output 4_PerCohortPreparations/output
</code></pre></div>    </div>
  </li>
  <li>
    <p>Inside each pipeline folder is the script template using name format
<code class="language-plaintext highlighter-rouge">submit_*_template.sh</code>
e.g. <code class="language-plaintext highlighter-rouge">eQTLGen_phase2/1_DataQC/DataQC/submit_DataQc_pipeline_template.sh</code>.
You should adjust this according to your data (e.g. specify the path
to your input folder, add required HPC modules), and save it. For
better tracking, I usually rename it too:
<code class="language-plaintext highlighter-rouge">eQTLGen_phase2/1_DataQC/DataQC/submit_DataQc_pipeline_EstBB_HT12v3.sh</code>.</p>
  </li>
</ul>

<p>❗ To ease the process, we have pre-filled some paths in the templates,
assuming that you use the recommended folder structure.</p>

<ul>
  <li>You should submit each pipeline from <em>inside</em> each pipeline folder.</li>
  <li>The logic of the cookbook is the following: the relevant output from
the previous step should be specified as the input for the next step
in the analysis.</li>
  <li>E.g. QCd genotype files from the output of first DataQC pipeline:
<code class="language-plaintext highlighter-rouge">eQTLGen_phase2/1_DataQC/output/[Your Cohort Name]/outputfolder_gen/gen_data_QCd/</code>
are the input for the second pipeline (imputation).</li>
</ul>

<h4 id="offline-use">Offline use</h4>

<p>In some HPCs the internet accessibility is restricted and this causes
error messages when trying to install Nextflow and running it. This is
because, by default, Nextflow executable tries to contact internet and
check if newer version of Nextflow is available. Also, by default, first
<a href="#1-data-qc">Data QC pipeline</a> tries to automatically download plink
executables and 1000G reference panel. In order to get the setup working
offline, please follow <a href="eQTLGen-p2-offline-instructions.html">these
instructions</a>. Please consult with
lead analysts, in case abovementioned instructions are not solving the
issues.</p>

<h4 id="running-monitoring-and-debugging-nextflow-pipelines">Running, monitoring and debugging Nextflow pipelines</h4>

<p>Example commands here are based on the Slurm scheduler. However, this
can be easily changed to other schedulers.</p>

<ul>
  <li>
    <p>In order to run Nextflow pipelines, you have to modify each template
script and load a couple of modules which enable to run Nextflow and
Singularity containers. These very common modules should be
available in your HPC, however exact name and version might differ:
<strong>Java &gt;=11</strong> and <strong>Singularity</strong>. You might also need a few
extra modules to run Singularity without problems. If you think this
might be the case, we recommend to check with your HPC documentation
and/or tech support which modules are required. E.g. in the
University of Tartu HPC there is also a module named <strong>squashFS</strong>
needed to run Singularity without problems.</p>
  </li>
  <li>
    <p>When you submit the job
e.g. <code class="language-plaintext highlighter-rouge">sbatch submit_DataQc_pipeline_EstBB_HT12v3.sh</code>, this initiates
the pipeline, makes the analysis environment (using Singularity
container) and automatically submits all the pipeline steps in the
correct order and parallelized way. A separate <code class="language-plaintext highlighter-rouge">work</code> directory is
made in the pipeline folder and will contain all the interim files
of the pipeline.</p>
  </li>
  <li>
    <p>Monitoring:</p>

    <ul>
      <li>
        <p>Monitor the <code class="language-plaintext highlighter-rouge">slurm-***.out</code> log file or the <code class="language-plaintext highlighter-rouge">.nextflow.log</code> file
and check if all the steps finish without error. Trick: command
<code class="language-plaintext highlighter-rouge">watch tail -n 20 slurm-***.out</code> helps you to interactively
monitor the status of the jobs.</p>
      </li>
      <li>
        <p>Use <code class="language-plaintext highlighter-rouge">squeue -u [YourUserName]</code> to see if individual tasks are in
the queue.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>If the pipeline has crashed (e.g. due to wall time), you can just
resubmit the same script after any required changes are implemented.
Nextflow does not rerun completed steps and continues only from the
steps which had not been completed.</p>
  </li>
  <li>
    <p>When the work has finished, download and check the job report, for
potential errors or warnings. This file is automatically written to
your output folder <code class="language-plaintext highlighter-rouge">pipeline_info</code> subfolder. E.g.
<code class="language-plaintext highlighter-rouge">output/pipeline_info/DataQcReport.html</code>.</p>
  </li>
  <li>
    <p>When you need to do some debugging, then you can use the last
section of the aforementioned report to figure out in which
subfolder from <code class="language-plaintext highlighter-rouge">work</code> folder the actual step was run. You can then
navigate to this folder and investigate the following hidden files:</p>

    <ul>
      <li>
        <p><code class="language-plaintext highlighter-rouge">.command.sh</code>: script which was submitted</p>
      </li>
      <li>
        <p><code class="language-plaintext highlighter-rouge">.command.log</code>: log file for seeing the analysis outputs/errors.</p>
      </li>
      <li>
        <p><code class="language-plaintext highlighter-rouge">.command.err</code>: file which lists the errors, if any.</p>
      </li>
    </ul>
  </li>
</ul>

<p>Hopefully those three files can give you some handle which might have
gone wrong. Contact us, if you think you need any help to resolve the
issue.</p>

<ul>
  <li>
    <p>We have specified reasonable computational resources for each
pipeline step. However, in some cases these might work differently,
depending on HPC or dataset size. Resources for each pipeline step
are specified in <code class="language-plaintext highlighter-rouge">config/base.conf</code>. In this file there are
resources (RAM, CPU, walltime) for each step of the pipeline, as
currently specified. You can try to increase those, if the pipeline
crashes due to insufficient resources. Also, you can adjust
cluster-specific options by modifying <code class="language-plaintext highlighter-rouge">clusterOptions</code> flag for each
process. For example you can modify
<code class="language-plaintext highlighter-rouge">clusterOptions = '--job-name=GenotypeQC'</code> into
<code class="language-plaintext highlighter-rouge">clusterOptions = '--job-name=GenotypeQC -p long'</code> to ensure that
this specific process runs on the partition named <code class="language-plaintext highlighter-rouge">long</code>.</p>
  </li>
  <li>
    <p>For some analyses, <code class="language-plaintext highlighter-rouge">work</code> directory can become quite large. So, when
the pipeline is successfully finished, and you have checked the
output, you can delete <code class="language-plaintext highlighter-rouge">work</code> directory, in order to save this
storage space of your HPC. But this means that the pipeline will
restart from scratch, if you ever need to rerun this pipeline.</p>
  </li>
  <li>
    <p>Nextflow and Singularity by default create cache folders in your
home directory. This may unexpectedly fill your home. Therefore, we
manually set and export the environment variables responsible for
cache folders within each nextflow pipeline to
<code class="language-plaintext highlighter-rouge">../../singularitycache</code> and <code class="language-plaintext highlighter-rouge">../../nextflowcache</code>. If you followed
the naming scheme from the <a href="#setup">setup</a> chapter and ran the
pipelines from within the pipeline directories, these evaluate to
<code class="language-plaintext highlighter-rouge">eQTLGen_phase2/singularitycache</code> and
<code class="language-plaintext highlighter-rouge">eQTLGen_phase2/nextflowcache</code>. Be aware that these might be changed
from your default if you’ve sourced the submit script from a
different directory.</p>
  </li>
</ul>

<h3 id="data">Data</h3>

<p>For this analysis plan you need the following information and data:</p>

<ul>
  <li>Name of your cohort.</li>
  <li>Unimputed array genotype data or WGS genotype data for your eQTL
samples.</li>
  <li>Gene expression matrix from your eQTL samples (for this project:
whole blood or PBMC).</li>
  <li>Genotype-to-expression linking file, linking sample IDs between
those two datasets.</li>
</ul>

<h4 id="name-of-your-cohort">Name of your cohort</h4>

<p>An informative name for your dataset is one of the required inputs for
the majority of the pipelines in this cookbook. This could be the
combination of cohort name and expression platform. If you contribute
with multiple datasets from e.g. several batches, you could specify it
by adding <code class="language-plaintext highlighter-rouge">_batch1</code>. If you contribute with multiple datasets from
several ancestries, then you could specify this by adding ancestry
e.g. <code class="language-plaintext highlighter-rouge">_EUR</code>.</p>

<p>Example 1: For EstBB cohort I have 2 datasets <code class="language-plaintext highlighter-rouge">EstBB_HT12v3</code> and
<code class="language-plaintext highlighter-rouge">EstBB_RNAseq</code>.</p>

<p>Example 2: If there would be two EstBB RNAseq datasets, I would use:
<code class="language-plaintext highlighter-rouge">EstBB_RNAseq_batch1</code> and <code class="language-plaintext highlighter-rouge">EstBB_RNAseq_batch2</code>.</p>

<h4 id="genotype-data">Genotype data</h4>

<ul>
  <li>We require either an unimputed PLINK dataset.
<a href="https://www.cog-genomics.org/plink/1.9/formats#bed">.bed</a>/<a href="https://www.cog-genomics.org/plink/1.9/formats#bim">.bim</a>/<a href="https://www.cog-genomics.org/plink/1.9/formats#fam">.fam</a>
or an unimputed VCF dataset. Can be either genotyping array-based or
WGS data. Can be in hg19/GRCh37 or hg38/GRCh38.</li>
  <li>WGS datasets are expected to have some sample-level quality control
previously done (such as mean sequence coverage, percent of chimeric
reads, median and standard deviation of insert size, contamination
rate, bacterial contamination, large chromosomal abnormalities).</li>
  <li>We encourage the use of a VCF file format for WGS data. This enables
the DataQC step to perform variant-level WGS-specific quality
control.</li>
  <li>A VCF dataset can either be split by chromosome (by using a globbing
pattern surrounded by single quotes), or it can be a single VCF.</li>
  <li>Ideally, the <code class="language-plaintext highlighter-rouge">.fam</code> file should contain information about the
reported sex of the sample. This information is used in the
automatic data quality control step. If this information is not
available, you can still use those files but this specific quality
control step will be skipped.</li>
  <li>For both VCF and Plink input, a separate .fam file can be supplied
with reported sex data. When Plink input is used, this separate .fam
file will take precedence over the one in the Plink dataset.</li>
  <li>In order to take advantage of optimal pre-phasing, the input
genotype data should ideally have several thousand genotype samples.
This means that if you have e.g. 7,000 samples in the available
genotype data and only 500 of those have expression available (eQTL
samples), please specify all 7,000 as the input of the analysis, not
only eQTL samples. A larger number of input genotype samples will
yield better pre-phasing and subsequent imputation. eQTL samples
will be filtered after the imputation step. Our pipelines will
automatically use up to 5,000 genotype samples in data QC and
imputation pipelines. If only eQTL samples have genotype data
available, only eQTL samples will be used.</li>
</ul>

<p>❗ Standard pre-imputation genotype and sample quality control will be
done by our pipeline, so no need for thorough QC.</p>

<h4 id="gene-expression-data">Gene expression data</h4>

<ul>
  <li>Tab-delimited expression matrix.</li>
  <li>Each row contains a gene expression value and each column contains a
sample ID.</li>
  <li>The first column has column name <code class="language-plaintext highlighter-rouge">-</code> and contains either ENSEMBL
gene IDs (RNA-seq), Illumina ArrayAddress IDs for probes or probe
names for corresponding Affymetrix array.</li>
  <li>For RNA-seq and Illumina arrays we expect that data is raw: not
normalized and/or transformed. Our pipeline takes care of those
steps.</li>
  <li>Can be gzipped.</li>
</ul>

<p>❗ For <strong>RNA-seq data</strong> we expect that the raw RNA-seq data has already
been appropriately processed and QC’d prior to generating the count
matrix (e.g. proper settings for the read mapping, etc.).</p>

<p>❗ For <strong>Affymetrix arrays</strong> we expect that <strong>all</strong> the standard data
preprocessing steps for this array type have already been applied
(e.g. RMA normalization). On top of those, our pipeline just applies
inverse normal transformation on each gene (forces the gene expression
distribution to be normal).</p>

<p>❗ For <strong>Affymetrix arrays</strong> we have currently used probe-to-gene
linking as used in eQTLGen phase 1. It is possible that you have some
other format for probe IDs: if this is the case, please contact us and
we will update the pipeline accordingly.</p>

<h4 id="genotype-to-expression-gte-linking-file">Genotype-to-expression (GTE) linking file</h4>

<ul>
  <li>Genotype-to-expression (GTE) linking file: tab-delimited file with
sample ID matches between genotype data (column 1) and expression
data (column 2). If those IDs are the same in both data layers, you
can just use file with same sample IDs in both columns.</li>
</ul>

<p>If you participated in eQTLGen phase 1 analyses, this file should
already be available.</p>

<h4 id="additional-covariates">Additional covariates</h4>

<p>As standard, within the dataQC step a number of default covariates are
automatically constructed from both genotype data and expression data.
Respectively, we calculate 10 genotype and 100 expression PCs from from
the the data. The genotype-inferred sex is used as an additional
covariate. Optionally, an additional set of covariates can be supplied,
if you know that these are relevant for given dataset. For that please
make a file with additional cohort-specific covariates:</p>

<ul>
  <li>Can be tab-delimited or space-delimited.</li>
  <li>The first column should be the sample identifier, labeled
<code class="language-plaintext highlighter-rouge">SampleID</code>.</li>
  <li>Subsequent columns constitute the additional covariate, with the
column name corresponding to the covariate name.</li>
  <li>Categorical variables need to be text-based (e.g. batch1, batch2,
etc), rather than numerical.</li>
  <li>Categorical variables with <em>k</em> categories over 2 (<em>k</em> &gt; 2) are
automatically split into binary variables with values of 0 or 1.</li>
  <li>Dichotomous text-based variables (<em>k</em> = 2) are similarly converted
to a variable with values of 0 or 1.</li>
</ul>

<h4 id="mixups">Mixups</h4>

<p>We assume that potential sample mixups have already been identified,
resolved or removed in the previous research. E.g. see details from
<a href="https://github.com/molgenis/systemsgenetics/wiki/eQTL-mapping-analysis-cookbook-(eQTLGen)#step-4---mixupmapper">here</a>.</p>

<h3 id="analysis-instructions">Analysis instructions</h3>

<h4 id="1-data-qc">1. Data QC</h4>

<p>In this step, we prepare the genotype and gene expression data for the
next steps. Specifically, the pipeline does the following steps:</p>

<ul>
  <li>Genotypes
    <ul>
      <li>WGS-specific QC
        <ul>
          <li>WGS-specific variant level QC (VQSR thresholds, PASS labels,
inbreeding coefficients, etc.)</li>
        </ul>
      </li>
      <li>Standard SNP QC filtering (call-rate&gt;0.95, Hardy-Weinberg
P&gt;1e-6, MAF&gt;0.01).</li>
      <li>Individual-level missingness filter &lt;0.05.</li>
      <li>Comparison of the reported and genetic check, removal of
mismatched samples.</li>
      <li>Removal of samples with unclear genetic sex.</li>
      <li>Removal of samples with excess heterozygosity (+/-3SD from the
mean).</li>
      <li>Removal of related samples (3rd degree relatives). From each
pair of related samples, one is kept in the data.</li>
      <li>Visualisation of samples in the genetic reference space,
instructions on how to remove or split the data in case of
multi-ancestry samples.</li>
      <li>Removal of in-sample genetic outliers.</li>
      <li>Calculates 10 first genetic principal components (PCs), used in
analyses as covariates to correct for population stratification.</li>
    </ul>
  </li>
  <li>Gene expression:
    <ul>
      <li>Aligns sample IDs between genotype samples and gene expression
samples.</li>
      <li>Filters in blood-expressed genes.</li>
      <li>Replaces array probe IDs with gene IDs.</li>
      <li>Iteratively checks for expression outliers and removes samples
which fail this check.</li>
      <li>Normalises the data according to the expression platform used
and applies additional inverse normal transformation.</li>
      <li>Calculates 100 first expression PCs, used in analyses as
covariates to correct for unknown variation.</li>
      <li>Calculates the expression summary statistics for each gene, used
to do QC and gene filtering in the meta-analysis.</li>
    </ul>
  </li>
  <li>Additional steps:
    <ul>
      <li>Removes samples whose genetic sex does not match with the
expression of sex-specific genes.</li>
      <li>Reorders the genotype samples into random order.</li>
      <li>Organises all the QCd data into the standard folder format.</li>
      <li>Provides commented html QC report which should be used to get an
overview of the quality of the data.</li>
    </ul>
  </li>
</ul>

<h5 id="input-files">Input files</h5>

<h6 id="genotype-data-1">Genotype data</h6>

<ul>
  <li>Unimputed genotype file in plink
<a href="https://www.cog-genomics.org/plink/1.9/input#bed">.bed/.bim/.fam</a>
format. It is advisable that the .fam file also includes observed
sex for all samples (format: males=1, females=2) so that pipeline is
able to compare this with genetically inferred sex. For samples with
missing reported sex, this step will be skipped.</li>
  <li>We also allow unimputed VCF datasets as input, specifically for WGS
datasets for which we can do WGS-specific variant level QC.</li>
  <li>We allow a separate .fam file to be supplied with reported sex
information, which is especially useful for VCF datasets.</li>
</ul>

<p>❗ Because pre-phasing of genotypes benefits from larger sample sizes
and many eQTL datasets have modest sample sizes (N&lt;1,000), it is not
advisable to prefilter the unimputed genotype data to include only eQTL
samples. Our pipeline will extract eQTL samples itself (based on the
genotype-to-expression file) and, <em>if</em> additional samples are available,
includes additional up to 5,000 genotype samples which will be kept
until the pre-phasing step of the analysis. If the genotype data is
available for &lt;5,000 samples, the pipeline uses only those samples
which are available.</p>

<h6 id="expression-data">Expression data</h6>

<ul>
  <li>Raw, unprocessed gene expression matrix. Tab-delimited file,
genes/probes in the rows, samples in the columns.</li>
  <li>First column has header “-”.</li>
  <li>For Illumina arrays, probe ID has to be Illumina ArrayAddress.</li>
  <li>For RNA-seq, gene ID has to be stable ENSEMBL gene ID (ENSEMBL v75).</li>
  <li>For Affymetrix arrays we expect that gene expression matrix has
already gone through the standard preprocessing and is in the same
format as was used in eQTLGen phase 1 analyses (incl. array probe
names).</li>
</ul>

<p>❗ For Affymetrix arrays- if you did not participate in eQTLGen phase 1,
it might be that your probe name format does not align with our
pipeline. Please contact with urmo.vosa at gmail.com and send some
information, how the probe naming and annotation was done in your data.
We will then update the pipeline accordingly.</p>

<p>❗ For Affymetrix arrays- if you did not participate in eQTLGen phase 1,
it might be that your probe name format does not align with our
pipeline. Please contact with urmo.vosa at gmail.com and send some
information, how the probe naming and annotation was done in your data.
We will then update the pipeline accordingly.</p>

<h6 id="additional-files">Additional files</h6>

<ul>
  <li>Genotype-to-expression linking file (GTE). Tab-delimited file, no
header, 2 columns: sample ID in genotype data, corresponding sample
ID in gene expression data. You can use this file to select samples
which go into the analysis.</li>
  <li>Optional file with cohort-specific covariates. If your cohort has
specific covariates which could be relevant, please specify those in
separate file.
    <ul>
      <li>Can be tab-delimited or space-delimited.</li>
      <li>The first column should be the sample identifier, labeled
<code class="language-plaintext highlighter-rouge">SampleID</code>.</li>
      <li>Subsequent columns constitute the additional covariate, with the
column name corresponding to the covariate name.</li>
      <li>Categorical variables need to be text-based (e.g. batch1,
batch2, etc), rather than numerical.</li>
      <li>Categorical variables with <em>k</em> categories over 2 (<em>k</em> &gt; 2)
are automatically split into binary variables with values of 0
or 1.</li>
      <li>Dichotomous text-based variables (<em>k</em> = 2) are similarly
converted to a variable with values of 0 or 1.</li>
    </ul>
  </li>
</ul>

<h5 id="instructions">Instructions</h5>

<ol>
  <li>Go into <code class="language-plaintext highlighter-rouge">eQTLGen_phase2/1_DataQC</code>.</li>
  <li>Clone the <code class="language-plaintext highlighter-rouge">DataQC</code> pipeline
<code class="language-plaintext highlighter-rouge">git clone https://github.com/eQTLGen/DataQC.git</code>.</li>
  <li>Make folder <code class="language-plaintext highlighter-rouge">output</code>.</li>
  <li>Go into pipeline folder <code class="language-plaintext highlighter-rouge">eQTLGen_phase2/1_DataQC/DataQC</code> and adjust
the script template with needed modules, input paths and settings.
Aside from the paths to your data files, you must specify:</li>
  <li>Expression platform of your expression data (<code class="language-plaintext highlighter-rouge">--exp_platform</code>).
Options: Illumina arrays: <code class="language-plaintext highlighter-rouge">HT12v3</code>, <code class="language-plaintext highlighter-rouge">HT12v4</code> or <code class="language-plaintext highlighter-rouge">HuRef8</code>; RNA-seq:
<code class="language-plaintext highlighter-rouge">RNAseq</code>; Affymetrix arrays: <code class="language-plaintext highlighter-rouge">AffyU219</code> or <code class="language-plaintext highlighter-rouge">AffyHumanExon</code>.</li>
  <li>Informative name for your dataset (<code class="language-plaintext highlighter-rouge">--cohort_name</code>): your unique
dataset name used by all pipelines.</li>
  <li>The genome build of your dataset (<code class="language-plaintext highlighter-rouge">--genome_build</code>): <code class="language-plaintext highlighter-rouge">hg19</code>,
<code class="language-plaintext highlighter-rouge">GRCh37</code>, <code class="language-plaintext highlighter-rouge">hg38</code> or <code class="language-plaintext highlighter-rouge">GRCh38</code> (<code class="language-plaintext highlighter-rouge">GRCh37</code> prefilled).</li>
  <li>If your dataset concerns a VCF dataset, replace the <code class="language-plaintext highlighter-rouge">--bfile</code>
argument with the <code class="language-plaintext highlighter-rouge">--vcf</code> argument. The <code class="language-plaintext highlighter-rouge">--vcf</code> argument expects a
full path to a vcf dataset. Pathname extension using globbing is
allowed (using <code class="language-plaintext highlighter-rouge">*</code> or <code class="language-plaintext highlighter-rouge">?</code>), but the path should be provided without
pathway extension.</li>
  <li>You can select scheduler type by adjusting the profile as following:</li>
</ol>

<ul>
  <li>Slurm: <code class="language-plaintext highlighter-rouge">-profile slurm,singularity</code></li>
  <li>PBS/TORQUE: <code class="language-plaintext highlighter-rouge">-profile pbs,singularity</code></li>
  <li>SGE: <code class="language-plaintext highlighter-rouge">-profile sge,singularity</code></li>
</ul>

<ol>
  <li>If your cluster setup does not allow internet connection, you can
specify paths to local plink executables and reference files
(arguments: <code class="language-plaintext highlighter-rouge">--plink_executable</code>, <code class="language-plaintext highlighter-rouge">--plink2_executable</code> and
<code class="language-plaintext highlighter-rouge">--reference_1000g_folder</code>). Contact lead analysts for further
guidance, if needed.</li>
</ol>

<p>This is the template for Slurm scheduler
(<code class="language-plaintext highlighter-rouge">submit_DataQC_pipeline_template.sh</code>):</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#!/bin/bash

#SBATCH --time=48:00:00
#SBATCH -N 1
#SBATCH --ntasks-per-node=1
#SBATCH --mem=6G
#SBATCH --mail-type=BEGIN
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL
#SBATCH --job-name="DataQc"

# These are needed modules in UT HPC to get singularity and Nextflow running. Replace with appropriate ones for your HPC.
module load java-1.8.0_40
module load singularity/3.5.3
module load squashfs/4.4

# If you follow the eQTLGen phase II cookbook and analysis folder structure,
# some of the following paths are pre-filled.
# https://github.com/eQTLGen/eQTLGen-phase-2-cookbook/wiki/eQTLGen-phase-II-cookbook

# We set the following variables for nextflow to prevent writing to your home directory (and potentially filling it completely)
# Feel free to change these as you wish.
export SINGULARITY_CACHEDIR=../../singularitycache
export NXF_HOME=../../nextflowcache

# Disable pathname expansion. Nextflow handles pathname expansion by itself.
set -f

# Define paths
nextflow_path=../../tools # folder where Nextflow executable is

# Genotype data
bfile_path=[full path to your input genotype files without .bed/.bim/.fam extension]

# Other data
exp_path=[full path to your gene expression matrix]
gte_path=[full path to your genotype-to-expression file]
exp_platform=[expression platform name: HT12v3/HT12v4/HuRef8/RNAseq/AffyU219/AffyHumanExon]
cohort_name=[name of the cohort]
genome_build="GRCh37"
output_path=../output # Output path

# Additional settings and optional arguments for the command

# --GenOutThresh [numeric threshold]
# --GenSdThresh [numeric threshold]
# --ExpSdThresh [numeric threshold]
# --ContaminationArea [number between 0 and 90, default 30]
# --AdditionalCovariates [file with additional covariates]
# --InclusionList [file with the list of samples to restrict the analysis]
# --ExclusionList [file with the list of samples to remove from the analysis]
# --preselected_sex_check_vars "data/Affy6_pruned_chrX_variant_positions.txt"
# --AdditionalCovariates [file with additional covariates. First column should be `SampleID`]
# --gen_qc_steps 'WGS'
# --fam [PLINK .fam file. Takes precedence over .fam file supplied with `--bfile`]
# --plink_executable [path to plink executable (PLINK v1.90b6.26 64-bit)]
# --plink2_executable [path to plink2 executable (PLINK v2.00a3.7LM 64-bit Intel)]
# --reference_1000g_folder [path to folder with 1000G reference data]

# Command:
NXF_VER=21.10.6 ${nextflow_path}/nextflow run DataQC.nf \
--bfile ${bfile_path} \
--expfile ${exp_path} \
--gte ${gte_path} \
--exp_platform ${exp_platform} \
--cohort_name ${cohort_name} \
--genome_build ${genome_build} \
--outdir ${output_path}  \
-profile slurm,singularity \
-resume
</code></pre></div></div>

<ol>
  <li>
    <p>Submit the job.</p>
  </li>
  <li>
    <p>Go into the output folder, download and investigate the thorough
report file <code class="language-plaintext highlighter-rouge">Report_DataQc_[your cohort name].html</code>. You will find
diagnostic plots and instructions on how to proceed from this
report.</p>
  </li>
  <li>
    <p>According to the instructions in the QC report, it is likely that
you need to adjust some of the QC thresholds. In your slurm script,
you can add the some or all of the arguments <code class="language-plaintext highlighter-rouge">--GenOutThresh</code>,
<code class="language-plaintext highlighter-rouge">--GenSdThresh</code>, <code class="language-plaintext highlighter-rouge">--ExpSdThresh</code>, <code class="language-plaintext highlighter-rouge">--ContaminationArea</code>, and specify
the appropriate values for each of those.</p>
  </li>
  <li>
    <p>In WGS datasets many samples might not fit the thresholds set on the
X-chromosome heterozygosity (F&lt;0.2 and F&gt;0.8). To resolve this
you can add the argument
<code class="language-plaintext highlighter-rouge">--preselected_sex_check_vars ${sex_check_vars_path}</code>, which will
make the method use a preselected set of variants selected based on
array data.</p>
  </li>
  <li>
    <p>In some cases, you might also need to split the data into several
batches, according to the ancestry. Then you should run this and
following pipelines for each of the batches separately.</p>
  </li>
  <li>
    <p>Resubmit the job.</p>
  </li>
  <li>
    <p>Go into the output folder, download and investigate the report file
<code class="language-plaintext highlighter-rouge">Report_DataQc_[your cohort name].html</code>. Now QC plots should look as
expected.</p>
  </li>
  <li>
    <p>If some of the plots still indicate issues, adjust the arguments and
re-run the pipeline. You might need to do so a couple of times until
there are no more apparent issues.</p>
  </li>
  <li>
    <p>🎉 Done!</p>
  </li>
</ol>

<h5 id="output">Output</h5>

<p>Pipeline gives output into the specified directory. The important files
are following.</p>

<ol>
  <li>Files: <code class="language-plaintext highlighter-rouge">output/outputfolder_gen/gen_data_QCd/*_ToImputation.bed</code>,
<code class="language-plaintext highlighter-rouge">output/outputfolder_gen/gen_data_QCd/*_ToImputation.bim</code>,
<code class="language-plaintext highlighter-rouge">output/outputfolder_gen/gen_data_QCd/*_ToImputation.fam</code> are the
filtered and QCd genotype files which need to be the input for the
next, <a href="#2-genotype-imputation">imputation pipeline</a>. You need to
specify the path to the <em>base</em> name (no extension <code class="language-plaintext highlighter-rouge">.bed/.bim/.fam</code>)
as an input.</li>
  <li>The whole <code class="language-plaintext highlighter-rouge">output</code> folder should be specified as one of the inputs
for <a href="#4-per-cohort-data-preparations">per-cohort preparations
pipeline</a>. This pipeline
automatically uses the processed, QCd expression data and covariate
file to run data encoding and partial derivative calculation. It
then organises the encoded matrices for sharing with the central
site. It also extracts some QC files, plots and summaries for
sharing with the central site.</li>
</ol>

<p>❗ While continuing with the next step, now it is also good time to send
the <code class="language-plaintext highlighter-rouge">Report_DataQc_[your cohort name].html</code> to Urmo Võsa (urmo.vosa at
gmail.com) and Robert Warmerdam (c.a.warmerdam at umcg.nl). This way we
can have a initial look to the quality of the data and let you know if
something seems unoptimal.</p>

<h4 id="2-genotype-imputation">2. Genotype imputation</h4>

<p>In this step we impute the QC’d genotype data to the recently released
1000G 30X WGS reference panel.</p>

<p>Specifically, this pipeline performs the following steps:</p>

<ul>
  <li>Lifts the unimputed genotype files to hg38/GRCh38 coordinates.
(depending on genome build of input data)</li>
  <li>Aligns unimputed genotypes to reference panel.</li>
  <li>Converts unimputed genotype files to <code class="language-plaintext highlighter-rouge">.vcf</code> format.</li>
  <li>Fixes alleles to match with the reference panel.</li>
  <li>Does another round of genotype QC (Hardy-Weinberg P-value &lt; 1e-6,
missingness &gt; 0.05, and minor allele frequency &lt; 0.01).</li>
  <li>Calculates individual-level missingness.</li>
  <li>Does genotype pre-phasing (with Eagle v2.4.1).</li>
  <li>Does genotype imputation (with Minimac4).</li>
  <li>Filters imputed genotypes to MAF&gt;0.01.</li>
</ul>

<h5 id="input-files-1">Input files</h5>

<ul>
  <li>Unimputed and QC’d genotype files in plink <code class="language-plaintext highlighter-rouge">.bed/.bim/.fam</code>. These
files are in the output of previous <a href="#1-data-qc">data QC pipeline</a>.
These are in the folder <code class="language-plaintext highlighter-rouge">output/outputfolder_gen/gen_data_QCd</code>. You
need to specify the path to the base name (no extension
<code class="language-plaintext highlighter-rouge">.bed/.bim/.fam</code>) as an input.</li>
  <li>Folder with all the needed reference files for pre-phasing and
imputation. We provide it in Dropbox (see below).</li>
  <li>Genotype-to-expression file.</li>
</ul>

<h5 id="instructions-1">Instructions</h5>

<ol>
  <li>
    <p>Go into <code class="language-plaintext highlighter-rouge">eQTLGen_phase2/2_Imputation</code>.</p>
  </li>
  <li>
    <p>Clone the <code class="language-plaintext highlighter-rouge">eQTLGenImpute</code> pipeline
<code class="language-plaintext highlighter-rouge">git clone https://github.com/eQTLGen/eQTLGenImpute.git</code>.</p>
  </li>
  <li>
    <p>Make folder <code class="language-plaintext highlighter-rouge">output</code>.</p>
  </li>
  <li>
    <p>Download the zipped reference
<code class="language-plaintext highlighter-rouge">wget https://www.dropbox.com/s/6g58ygjg9d2fvbi/eQTLGenReferenceFiles.tar.gz?dl=1</code>.</p>
  </li>
  <li>
    <p>Because this file is ~30GB and can corrupt during the download, also
please download corresponding md5sum file
<code class="language-plaintext highlighter-rouge">wget https://www.dropbox.com/s/ekfciajzevn6o1l/eQTLGenReferenceFiles.tar.gz.md5?dl=1</code>.</p>
  </li>
  <li>
    <p>Check if download was successful
<code class="language-plaintext highlighter-rouge">md5sum --check eQTLGenReferenceFiles.tar.gz.md5</code>. You should see
this message in your terminal: <code class="language-plaintext highlighter-rouge">eQTLGenReferenceFiles.tar.gz:OK</code>.</p>
  </li>
  <li>
    <p>Unzip the reference file <code class="language-plaintext highlighter-rouge">tar -xfvz eQTLGenReferenceFiles.tar.gz</code>.
This yields the folder named <code class="language-plaintext highlighter-rouge">hg38</code> which contains all needed
reference files.</p>
  </li>
  <li>
    <p>Go into <code class="language-plaintext highlighter-rouge">eQTLGen_phase2/2_Imputation/eQTLGenImpute</code> pipeline folder
and adjust the imputation script template with needed modules and
inputs</p>
  </li>
  <li>
    <p>Path to QCd genotype data: path pointing to unimputed and QC’d
genotype files in plink <code class="language-plaintext highlighter-rouge">.bed/.bim/.fam</code> format, ending with
<code class="language-plaintext highlighter-rouge">*_ToImputation</code>. These files are in the output of previous <a href="#1-data-qc">data QC
pipeline</a>, ending with <code class="language-plaintext highlighter-rouge">*_ToImputation.*</code> and are in the
folder <code class="language-plaintext highlighter-rouge">output/outputfolder_gen/gen_data_QCd</code>. This path is
pre-filled.</p>
  </li>
  <li>
    <p>Path to reference folder (pre-filled).</p>
  </li>
  <li>
    <p>Output path: to the folder <code class="language-plaintext highlighter-rouge">output</code> you made (pre-filled).</p>
  </li>
  <li>
    <p>Cohort name: this should be the same as in <code class="language-plaintext highlighter-rouge">DataQC</code>.</p>
  </li>
  <li>
    <p>Path to the folder with reference files (pre-filled).</p>
  </li>
  <li>
    <p>The genome build of your dataset (<code class="language-plaintext highlighter-rouge">--genome_build</code>): <code class="language-plaintext highlighter-rouge">hg19</code>,
<code class="language-plaintext highlighter-rouge">GRCh37</code>, <code class="language-plaintext highlighter-rouge">hg38</code> or <code class="language-plaintext highlighter-rouge">GRCh38</code> (hg19/GRCh37 by default).</p>
  </li>
  <li>
    <p>You can select scheduler type by adjusting the profile as following:</p>

    <ul>
      <li>Slurm: <code class="language-plaintext highlighter-rouge">-profile slurm,singularity</code></li>
      <li>PBS/TORQUE: <code class="language-plaintext highlighter-rouge">-profile pbs,singularity</code></li>
      <li>SGE: <code class="language-plaintext highlighter-rouge">-profile sge,singularity</code></li>
    </ul>
  </li>
</ol>

<p>This is the template for Slurm scheduler
(<code class="language-plaintext highlighter-rouge">submit_imputation_pipeline_template.sh</code>):</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#!/bin/bash

#SBATCH --time=72:00:00
#SBATCH -N 1
#SBATCH --ntasks-per-node=1
#SBATCH --mem=6G
#SBATCH --mail-type=BEGIN
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL
#SBATCH --job-name="ImputeGenotypes"

# These are needed modules in UT HPC to get Singularity and Nextflow running.
# Replace with appropriate ones for your HPC.
module load java/11.0.2
module load singularity/3.5.3
module load squashfs/4.4

# If you follow the eQTLGen phase II cookbook and analysis folder structure,
# some of the following paths are pre-filled.
# https://github.com/eQTLGen/eQTLGen-phase-2-cookbook/wiki/eQTLGen-phase-II-cookbook

# We set the following variables for nextflow to prevent writing to your home directory (and potentially filling it completely)
# Feel free to change these as you wish.
export SINGULARITY_CACHEDIR=../../singularitycache
export NXF_HOME=../../nextflowcache

# Define paths and arguments
nextflow_path=../../tools # folder where Nextflow executable is.
reference_path=../hg38 # folder where you unpacked the reference files.

cohort_name=[name of your cohort]
qc_input_folder=../../1_DataQC/output/ # folder with QCd genotype and expression data, output of DataQC pipeline.
output_path=../output/ # Output path.
genome_build="GRCh37"

# Command
NXF_VER=21.10.6 ${nextflow_path}/nextflow run eQTLGenImpute.nf \
--qcdata ${qc_input_folder} \
--target_ref ${reference_path}/genome_reference/Homo_sapiens.GRCh38.dna.primary_assembly.fa \
--ref_panel_hg38 ${reference_path}/harmonizing_reference/30x-GRCh38_NoSamplesSorted \
--eagle_genetic_map ${reference_path}/phasing_reference/genetic_map/genetic_map_hg38_withX.txt.gz \
--eagle_phasing_reference ${reference_path}/phasing_reference/phasing/ \
--minimac_imputation_reference ${reference_path}/imputation_reference/ \
--cohort_name ${cohort_name} \
--genome_build ${genome_build} \
--outdir ${output_path}  \
-profile slurm,singularity \
-resume
</code></pre></div></div>

<ol>
  <li>
    <p>Submit the job.</p>
  </li>
  <li>
    <p>Check the <code class="language-plaintext highlighter-rouge">output/pipeline_info/imputation_report.html</code>.</p>
  </li>
  <li>
    <p>🎉 Done!</p>
  </li>
</ol>

<h5 id="output-1">Output</h5>

<p>The pipeline gives output into the specified directory. The important
files are following.</p>

<ol>
  <li>Folder <code class="language-plaintext highlighter-rouge">output/postimute/</code> should be specified as the input for the
next step, <a href="#3-genotype-conversion">genotype conversion</a>. This
folder contains imputed <code class="language-plaintext highlighter-rouge">.vcf.gz</code> files, filtered by MAF&gt;0.01.</li>
  <li>Folder <code class="language-plaintext highlighter-rouge">output/preimpute/</code> contains the formatted, filtered, and QCd
genotype data before imputation (<code class="language-plaintext highlighter-rouge">.vcf.gz</code>). This is not directly
needed for this cookbook and might be used for other applications,
debugging the imputation, or just deleted.</li>
</ol>

<h4 id="3-genotype-conversion">3. Genotype conversion</h4>

<p>In this step, we convert the imputed genotype files from <code class="language-plaintext highlighter-rouge">.vcf.gz</code>
format into efficient <code class="language-plaintext highlighter-rouge">.hdf5</code> format. This file format is native to HASE
and is needed for running the following HASE steps (encoding, partial
derivative computation) in a more efficient way.</p>

<p>Specifically, this pipeline performs the following steps:</p>

<ul>
  <li>It chunks <code class="language-plaintext highlighter-rouge">.vcf.gz</code> files into chunks of 25,000 variants for faster
parallel processing.</li>
  <li>It converts the chunks into <code class="language-plaintext highlighter-rouge">.hdf5</code> format.</li>
  <li>It outputs a file with summary and quality metrics for each SNP
included in the analysis. This includes MAF, Hardy-Weinberg P-value,
Mach R2 (a measure of imputation quality), genotype counts, call
rate (always 1 as imputed data) and an indicator whether SNP was
typed of imputed.</li>
</ul>

<blockquote>
  <p>These quality metrics will be used for performing additional
per-variant QC in the central site.</p>
</blockquote>

<ul>
  <li>It renames and organises files into custom hdf5 genotype folder
format so that these are usable in the HASE framework.</li>
</ul>

<h5 id="input-files-2">Input files</h5>

<ul>
  <li>Folder containing imputed genotype files in the bgzipped <code class="language-plaintext highlighter-rouge">vcf.gz</code>
format.</li>
</ul>

<h5 id="instructions-2">Instructions</h5>

<ol>
  <li>
    <p>Go into <code class="language-plaintext highlighter-rouge">eQTLGen_phase2/3_ConvertVcf2Hdf5</code>.</p>
  </li>
  <li>
    <p>Clone the <code class="language-plaintext highlighter-rouge">ConvertVcf2Hdf5</code> pipeline
<code class="language-plaintext highlighter-rouge">git clone https://github.com/eQTLGen/ConvertVcf2Hdf5.git</code>.</p>
  </li>
  <li>
    <p>Make folder <code class="language-plaintext highlighter-rouge">output</code></p>
  </li>
  <li>
    <p>Go into <code class="language-plaintext highlighter-rouge">3_ConvertVcf2Hdf5/ConvertVcf2Hdf5</code> and adjust the genotype
conversion script template with needed modules and inputs.</p>
  </li>
  <li>
    <p>Imputed genotype path (pre-filled).</p>
  </li>
  <li>
    <p>Output path (pre-filled).</p>
  </li>
  <li>
    <p>You can select scheduler type by adjusting the profile as following:</p>

    <ul>
      <li>Slurm: <code class="language-plaintext highlighter-rouge">-profile slurm,singularity</code></li>
      <li>PBS/TORQUE: <code class="language-plaintext highlighter-rouge">-profile pbs,singularity</code></li>
      <li>SGE: <code class="language-plaintext highlighter-rouge">-profile sge,singularity</code></li>
    </ul>
  </li>
</ol>

<p>This is the template for Slurm scheduler
(<code class="language-plaintext highlighter-rouge">submit_genotype_conversion_template.sh</code>):</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#!/bin/bash

#SBATCH --time=24:00:00
#SBATCH -N 1
#SBATCH --ntasks-per-node=1
#SBATCH --mem=5G
#SBATCH --job-name="ConvertVcf2Hdf5"

# These are needed modules in UT HPC to get Singularity and Nextflow running.
# Replace with appropriate ones for your HPC.
module load java/11.0.2
module load singularity/3.5.3
module load squashfs/4.4

# If you follow the eQTLGen phase II cookbook and analysis folder structure,
# some of the following paths are pre-filled.
# https://github.com/eQTLGen/eQTLGen-phase-2-cookbook/wiki/eQTLGen-phase-II-cookbook

# We set the following variables for nextflow to prevent writing to your home directory (and potentially filling it completely)
# Feel free to change these as you wish.
export SINGULARITY_CACHEDIR=../../singularitycache
export NXF_HOME=../../nextflowcache

nextflow_path=../../tools/  # Path to Nextflow executable, no need to adjust if folder structure is same as recommended in cookbook.

cohortname=[name of your cohort]
genopath=../../2_Imputation/output/postimpute   # Folder with input genotype files in .vcf.gz format, no need to adjust if the folder structure is same as recommended in cookbook.
outputpath=../output/ # Path to output folder, no need to adjust if the folder structure is same as recommended in cookbook.

NXF_VER=21.10.6 ${nextflow_path}/nextflow run ConvertVcf2Hdf5.nf \
--vcf ${genopath} \
--outdir ${outputpath} \
--cohort_name ${cohortname} \
-profile slurm,singularity \
-resume
</code></pre></div></div>

<ol>
  <li>
    <p>Submit the job.</p>
  </li>
  <li>
    <p>Check the <code class="language-plaintext highlighter-rouge">output/pipeline_info/ConvertGenotype_report.html</code>.</p>
  </li>
  <li>
    <p>🎉 Done!</p>
  </li>
</ol>

<h5 id="output-2">Output</h5>

<p>After successful completion of the pipeline, there should be <code class="language-plaintext highlighter-rouge">hdf5</code>
genotype file structure in your output folder, which, in addition to
<code class="language-plaintext highlighter-rouge">pipeline_info</code>, contains folders named <code class="language-plaintext highlighter-rouge">individuals</code>, <code class="language-plaintext highlighter-rouge">probes</code>,
<code class="language-plaintext highlighter-rouge">genotype</code> and <code class="language-plaintext highlighter-rouge">SNPQC</code>. For eQTLGen phase 2 analyses, this folder is one
out of two inputs of <a href="#4-per-cohort-data-preparations">per-cohort data
preparations</a> pipeline.</p>

<h4 id="4-per-cohort-data-preparations">4. Per-cohort data preparations</h4>

<p>This step prepares the data and performs all the steps which are needed
for running encoded HASE meta-analysis in the central site.</p>

<p>Specifically, this pipeline performs the following steps:</p>

<ul>
  <li>Uses 1000G 30x reference file to create variant mapper files (to
make the SNPs from different studies jointly analyzable in central
site).</li>
  <li>Encodes genotype and gene expression data, and deletes the random
matrix used for encoding. This means that no information for
individual study participant is obtainable from encoded data, even
for the original cohort analyst.</li>
  <li>Calculates partial derivatives, needed for running the eQTL mapping.</li>
  <li>Permutes the sample links on the unencoded data, encodes, and
calculates partial derivatives for the permuted data. This is needed
for obtaining in-sample LD estimates for downstream analyses in the
central site (useful for e.g. multiple testing corrections and
fine-mapping).</li>
  <li>Associates expression PCs with genotypes, writes out suggestive
associations (P&lt;1×10<sup>-5</sup>). This enables us to make an
informed decision which covariates to include into encoded HASE
model in the central site and control for the collider effects.</li>
  <li>Replaces original sample IDs in the encoded data with
“CohortName__index”.</li>
  <li>Collects several summary reports, QC reports and diagnostic plots
from the output of <a href="#1-data-qc">data QC pipeline</a>.</li>
  <li>Collects summary file with SNP QC information (MAF, imputation
quality, genotype counts, imputation/typed status).</li>
  <li>Organises all the partial derivates, encoded matrices and QC reports
into the structured folder structure, ready for sharing.</li>
  <li>Calculates <code class="language-plaintext highlighter-rouge">md5sum</code> for all the shared files, so the integrity of
the upload can be checked.</li>
</ul>

<h5 id="input-files-3">Input files</h5>

<ul>
  <li>Folder with genotype files in the <code class="language-plaintext highlighter-rouge">.hdf5</code> format. This folder is the
output of <a href="#3-genotype-conversion">genotype conversion pipeline</a>.</li>
  <li>Full path to the output folder of <a href="#1-data-qc">data QC pipeline</a>.
Pipeline automatically takes preprocessed expression matrix,
covariate file and several QC metric files from this folder
structure.</li>
</ul>

<h5 id="instructions-3">Instructions</h5>

<ol>
  <li>
    <p>Go into <code class="language-plaintext highlighter-rouge">eQTLGen_phase2/4_PerCohortPreparations</code>.</p>
  </li>
  <li>
    <p>Make folder <code class="language-plaintext highlighter-rouge">output</code></p>
  </li>
  <li>
    <p>Clone the <code class="language-plaintext highlighter-rouge">PerCohortPreparations</code> pipeline
<code class="language-plaintext highlighter-rouge">git clone https://github.com/eQTLGen/PerCohortDataPreparatons.git</code>.</p>
  </li>
  <li>
    <p>Go into <code class="language-plaintext highlighter-rouge">4_PerCohortPreparations/PerCohortPreparations</code>.</p>
  </li>
  <li>
    <p>Put genotype reference file into
<code class="language-plaintext highlighter-rouge">4_PerCohortPreparations/PerCohortPreparations/bin/hase/data/</code>
folder. This was already downloaded together with imputation
references in <a href="#2-genotype-imputation">Imputation step</a>. Therefore
you can copy it to the correct location like that:
<code class="language-plaintext highlighter-rouge">cp ../../2_Imputation/hg38/hase_reference/* bin/hase/data/.</code>.</p>
  </li>
  <li>
    <p>Adjust the template script.</p>
  </li>
  <li>
    <p>Path to the genotypes in hdf5 format (pre-filled)</p>
  </li>
  <li>
    <p>Path to QC’d expression data (pre-filled)</p>
  </li>
  <li>
    <p>Output path (pre-filled)</p>
  </li>
  <li>
    <p>You can select scheduler type by adjusting the profile as following:</p>

    <ul>
      <li>Slurm: <code class="language-plaintext highlighter-rouge">-profile slurm,singularity</code></li>
      <li>PBS/TORQUE: <code class="language-plaintext highlighter-rouge">-profile pbs,singularity</code></li>
      <li>SGE: <code class="language-plaintext highlighter-rouge">-profile sge,singularity</code></li>
    </ul>
  </li>
</ol>

<p>This is the template script for Slurm scheduler
(<code class="language-plaintext highlighter-rouge">submit_per_cohort_preparations_template.sh</code>):</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#!/bin/bash

#SBATCH --time=24:00:00
#SBATCH -N 1
#SBATCH --ntasks-per-node=1
#SBATCH --mem=5G
#SBATCH --job-name="RunDataPreparations"

# These are needed modules in UT HPC to get Singularity and Nextflow running.
# Replace with appropriate ones for your HPC.
module load java/11.0.2
module load singularity/3.5.3
module load squashfs/4.4

# If you follow the eQTLGen phase II cookbook and analysis folder structure,
# some of the following paths are pre-filled.
# https://github.com/eQTLGen/eQTLGen-phase-2-cookbook/wiki/eQTLGen-phase-II-cookbook

# We set the following variables for nextflow to prevent writing to your home directory (and potentially filling it completely)
# Feel free to change these as you wish.
export SINGULARITY_CACHEDIR=../../singularitycache
export NXF_HOME=../../nextflowcache

nextflow_path=../../tools

genotypes_hdf5=../../3_ConvertVcf2Hdf5/output # Folder with genotype files in .hdf5 format
qc_data_folder=../../1_DataQC/output # Folder containing QCd data, inc. expression and covariates
output_path=../output

NXF_VER=21.10.6 ${nextflow_path}/nextflow run PerCohortDataPreparations.nf \
--hdf5 ${genotypes_hdf5} \
--qcdata ${qc_data_folder} \
--outdir ${output_path} \
-profile slurm,singularity \
-resume
</code></pre></div></div>

<ol>
  <li>
    <p>Submit the job.</p>
  </li>
  <li>
    <p>Check the
<code class="language-plaintext highlighter-rouge">output/pipeline_info/PerCohortDataPreparations_report.html</code>.</p>
  </li>
  <li>
    <p>🎉 Done!</p>
  </li>
</ol>

<h5 id="output-3">Output</h5>

<ul>
  <li>In the <code class="language-plaintext highlighter-rouge">output</code> folder there is subfolder called
<code class="language-plaintext highlighter-rouge">[YourCohortName]_IntermediateFilesEncoded_to_upload</code>. This folder
contains all the non-personal files, logs, reports and should be
shared with the central site.</li>
  <li>In the <code class="language-plaintext highlighter-rouge">output</code> folder there is also file
<code class="language-plaintext highlighter-rouge">[YourCohortName]_IntermediateFilesEncoded_to_upload.md5</code>. This
should also be shared with the central site, in order to check the
integrity of uploaded files.</li>
</ul>

<h4 id="5-share-the-data-with-central-location">5. Share the data with central location</h4>

<p>The instructions for getting the SFTP account in UT HPC: send an e-mail
to urmo.vosa at gmail.com and we will arrange the account. We will also
instruct how to upload the needed data to SFTP server.</p>

<p>The output folder from <a href="#4-per-cohort-data-preparations">per-cohort preparations
pipeline</a> named
<code class="language-plaintext highlighter-rouge">[YourCohortName]_IntermediateFilesEncoded_to_upload</code> and its
accompanying <code class="language-plaintext highlighter-rouge">.md5</code> file should be uploaded into our sftp server.</p>

<p>❗ You might want to inform your HPC team about this project and the
planned upload. Although the sheer file size is much smaller that would
be for the classical meta-analysis, it is still ~50GB in case of 500
samples and RNA-seq. For the few largest datasets, the upload might be
up to 500GB and might raise some red flags when monitoring the data
traffic ;).</p>

<p>🎉 Thank you, you have shared the data necessary for global eQTL
mapping! We will keep you updated and will re-contact if any additional
information is needed.</p>

<h3 id="central-analyses">Central analyses</h3>

<p>Following pipelines are for performing global <em>trans</em>-eQTL
<strong>meta</strong>-analysis in the central location and are here outlined FYI. If
you are cohort analyst, no further action is needed. However, feel free
to use this code and instructions to conduct further analyses in your
respective cohort.</p>

<p><a href="https://gitlab.com/eqtlgen-group/metaanalysis">Pipeline for running encoded
meta-analysis</a> - this is
for running encoded eQTL meta-analysis on one or several datasets.</p>

<p><a href="https://gitlab.com/eqtlgen-group/ExtractMetaAnalysisResults">Pipeline for extracting subsets of data from full summary
statistics</a> -
this is for extracting data subsets for subsequent interpretation.</p>

<h3 id="runtime-estimates">Runtime estimates</h3>

<p>Here are some runtime benchmarks for each of the steps in this cookbook:
these might help you to get some hint how much time running the
pipelines might take.</p>

<p>All benchmarks are for RNA-seq dataset with following features:</p>

<ul>
  <li>Final eQTL sample size of 477 samples.</li>
  <li>RNA-seq: up to 19,942 genes in processed expression matrix.</li>
  <li>GSA genotyping array: 700,078 variants in unimputed data.</li>
</ul>

<h4 id="data-qc">Data QC</h4>

<ul>
  <li>Initial number of samples in unimputed <code class="language-plaintext highlighter-rouge">.bed</code> genotype file: 1,052</li>
  <li>Initial number of SNPs in unimputed <code class="language-plaintext highlighter-rouge">.bed</code> genotype file: 700,078</li>
  <li>Initial number of samples in gene expression data: 1,074</li>
  <li>Initial number of gene in gene expression data: 48803</li>
  <li>Infrastructure: University HPC with ~150 compute nodes</li>
  <li>Scheduler: Slurm</li>
  <li>Dependency management: Singularity</li>
  <li>Time to run the pipeline: 25m</li>
  <li>CPU hours: 0.7h</li>
  <li>Final size of <code class="language-plaintext highlighter-rouge">output</code> directory: 344MB</li>
  <li>Final size of <code class="language-plaintext highlighter-rouge">work</code> subdirectory: 1.8GB</li>
</ul>

<p>Note: benchmarks to initial run with default settings, you probably need
to re-run at least once after adjusting the settings.</p>

<h4 id="imputation">Imputation</h4>

<ul>
  <li>Initial number of samples in QCd and unimputed <code class="language-plaintext highlighter-rouge">.bed</code> genotype file:</li>
  <li>Initial number of SNPs in QCd and unimpuated <code class="language-plaintext highlighter-rouge">.bed</code> genotype file:</li>
  <li>Infrastructure: University HPC with ~150 compute nodes</li>
  <li>Scheduler: Slurm</li>
  <li>Dependency management: Singularity</li>
  <li>Time to run the pipeline: ~4.5h</li>
  <li>CPU hours: ~80h</li>
  <li>Final size of <code class="language-plaintext highlighter-rouge">output</code> directory: ~13GB</li>
  <li>Final size of <code class="language-plaintext highlighter-rouge">work</code> subdirectory: ~100GB</li>
</ul>

<h4 id="genotype-conversion">Genotype conversion</h4>

<ul>
  <li>Number of samples in imputed <code class="language-plaintext highlighter-rouge">.vcf.gz</code> genotype data: ~4,000</li>
  <li>Number of variants in imputed <code class="language-plaintext highlighter-rouge">.vcf.gz</code> genotype data: ~4,000</li>
  <li>Infrastructure: University HPC with ~150 compute nodes</li>
  <li>Dependency management: Singularity</li>
  <li>Time to run the pipeline (without wall times): ~4.5h</li>
  <li>CPU hours: ~80h</li>
  <li>Final size of <code class="language-plaintext highlighter-rouge">output</code> directory: ~4GB</li>
  <li>Final size of <code class="language-plaintext highlighter-rouge">work</code> subdirectory: ~100GB</li>
</ul>

<h4 id="per-cohort-data-preparation">Per-cohort data preparation</h4>

<ul>
  <li>Infrastructure: University HPC with ~150 compute nodes</li>
  <li>Scheduler: Slurm</li>
  <li>Dependency management: Singularity</li>
  <li>Time to run the pipeline (without wall times): ~1.5h</li>
  <li>CPU hours: ~44.8h</li>
  <li>Final size of <code class="language-plaintext highlighter-rouge">output</code> directory: ~50GB</li>
  <li>Final size of <code class="language-plaintext highlighter-rouge">work</code> subdirectory: ~100GB</li>
</ul>

<h2 id="acknowledgements">Acknowledgements</h2>

<p>This cookbook utilizes HASE (<a href="https://github.com/roshchupkin/hase">https://github.com/roshchupkin/hase</a>) and
some of its helper scripts originally developed by:</p>

<p>Gennady V. Roscupkin (Department of Epidemiology, Radiology and Medical
Informatics, Erasmus MC, Rotterdam, Netherlands)</p>

<p>Hieab H. Adams (Department of Epidemiology, Erasmus MC, Rotterdam,
Netherlands).</p>

<h3 id="changes-from-the-original-hase-repo">Changes from the original HASE repo</h3>

<p>Robert Warmerdam (Department of Genetics, University Medical Center
Groningen, University of Groningen, Groningen, Netherlands) modified the
original HASE and fixed some bugs. He also added <code class="language-plaintext highlighter-rouge">classic-meta</code> option
to HASE analyses which enables to perform (encoded) inverse-variance
weighted meta-analysis and has started implementing methods for encoded
interaction analysis.</p>

<p>Urmo Võsa (Institute of Genomics, University of Tartu, Tartu, Estonia)
incorporated it into Nextflow pipeline and applied some minor
customization to the parts of the code.</p>

<p><strong>Changes:</strong></p>

<ul>
  <li>Fixed bug causing an exception when more than 1000 individuals were
used.</li>
  <li>Resolved bug causing the <code class="language-plaintext highlighter-rouge">--intercept</code> option having no effect.</li>
  <li>Made version numbers of pip packages explicit.</li>
  <li>Added commentary to code in places.</li>
  <li>Lines 355-357 of hase.py were commented out because this caused
pipeline to crash when &gt;1 datasets were added.</li>
  <li>Line 355 of /hdgwas/data.py were changed
<code class="language-plaintext highlighter-rouge">self.chunk_size=10000 --&gt; self.chunk_size=20000</code>.</li>
  <li>For eQTLGen pipelines: removed folders with unit tests and test
data, in order to keep the tool lightweight.</li>
</ul>

<h3 id="citation">Citation</h3>

<p>Original method paper for HASE framework:</p>

<p><a href="https://www.nature.com/articles/srep36076">Roshchupkin, G. V., H. H.H. Adams, M. W. Vernooij, A. Hofman, C. M. Van
Duijn, M. A. Ikram, and W. J. Niessen. “HASE: Framework for Efficient
High-Dimensional Association Analyses.” Scientific Reports 6, no. 1
(December 26, 2016): 36076.
https://doi.org/10.1038/srep36076.</a></p>

<p>1000G 30X imputation reference:</p>

<p><a href="https://www.sciencedirect.com/science/article/pii/S0092867422009916">Byrska-Bishop, Marta, Uday S. Evani, Xuefang Zhao, Anna O. Basile,
Haley J. Abel, Allison A. Regier, André Corvelo, et al. “High-Coverage
Whole-Genome Sequencing of the Expanded 1000 Genomes Project Cohort
Including 602 Trios.” Cell 185, no. 18 (September 1, 2022):
3426-3440.e19.
https://doi.org/10.1016/j.cell.2022.08.004.</a></p>

<p>Nextflow workflow management tool:</p>

<p><a href="https://www.nature.com/articles/nbt.3820">Di Tommaso, Paolo, Maria Chatzou, Evan W Floden, Pablo Prieto Barja,
Emilio Palumbo, and Cedric Notredame. “Nextflow Enables Reproducible
Computational Workflows.” Nature Biotechnology 35, no. 4 (2017): 316–19.
https://doi.org/10.1038/nbt.3820.</a></p>

<h3 id="contacts">Contacts</h3>

<p>For this cookbook: Urmo Võsa (urmo.vosa at gmail.com) and Robert
Warmerdam (c.a.warmerdam at umcg.nl)</p>

<p>For the method of HASE, please find contacts from here:
<a href="https://github.com/roshchupkin/hase">https://github.com/roshchupkin/hase</a></p>

</div>
<footer class="pt-5">
    <hr>
    <div class="container text-center">
        <img src="figs/genoomika.png" height="100px">
        <img src="figs/rug.jpg" height="100px">
        <p class="m-0 text-center text-white">Copyright &copy; eQTLGen 2022</p>
    </div>
    <!-- /.container -->
</footer>

<!-- Bootstrap core JavaScript -->
<script src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
        integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN"
        crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.11.0/umd/popper.min.js"
        integrity="sha384-b/U6ypiBEHpOf/4+1nzFpr53nxSS+GLCkfwBdFNTxtclqqenISfwAzpKaMNFNmj4"
        crossorigin="anonymous"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/js/bootstrap.min.js"
        integrity="sha384-h0AbiXch4ZDo7tp9hKZ4TsHbi047NrKGLO3SEJAg45jXxnGIfYzk4Si90RDIqNm1"
        crossorigin="anonymous"></script>

</body>

</html>
